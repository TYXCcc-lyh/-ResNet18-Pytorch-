# 02_æ¨¡å‹è®­ç»ƒ.py
"""
æ¨¡å‹è®­ç»ƒæ¨¡å— - ä½¿ç”¨PyTorchå’ŒResNet18è®­ç»ƒç™½å†…éšœç­›æŸ¥æ¨¡å‹

"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from torchvision import models, transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as T

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import time
import copy
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥tqdmï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æç¤ºå®‰è£…
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    print("âš ï¸ è­¦å‘Š: tqdm æ¨¡å—æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–è¿›åº¦æ¡")
    print("   è¯·è¿è¡Œ: pip install tqdm")
    TQDM_AVAILABLE = False
    
    # åˆ›å»ºç®€å•çš„è¿›åº¦æ¡æ›¿ä»£
    class SimpleProgressBar:
        def __init__(self, iterable=None, desc=None):
            self.iterable = iterable
            self.desc = desc
            if desc:
                print(desc)
        
        def __iter__(self):
            for i, item in enumerate(self.iterable):
                if i % 10 == 0:
                    print(f"  å¤„ç†ä¸­: {i}/{len(self.iterable)}", end='\r')
                yield item
            print(f"  å¤„ç†å®Œæˆ: {len(self.iterable)}/{len(self.iterable)}")
        
        def __len__(self):
            return len(self.iterable)
    
    tqdm = SimpleProgressBar

def get_script_dir():
    """è·å–è„šæœ¬æ‰€åœ¨ç›®å½•"""
    return os.path.dirname(os.path.abspath(__file__))

class CataractDataset(Dataset):
    """ç™½å†…éšœæ•°æ®é›†ç±»"""
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.classes = ['cataract', 'normal']
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}
        
        self.images = []
        self.labels = []
        
        # éå†æ‰€æœ‰ç±»åˆ«
        for class_name in self.classes:
            class_dir = os.path.join(root_dir, class_name)
            if not os.path.exists(class_dir):
                print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°ç±»åˆ«ç›®å½• {class_dir}")
                continue
                
            img_files = [f for f in os.listdir(class_dir) 
                        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]
            
            if not img_files:
                print(f"âš ï¸ è­¦å‘Š: ç±»åˆ« {class_name} ç›®å½•ä¸­æ²¡æœ‰å›¾ç‰‡æ–‡ä»¶")
                continue
                
            for img_name in img_files:
                self.images.append(os.path.join(class_dir, img_name))
                self.labels.append(self.class_to_idx[class_name])
        
        if len(self.images) == 0:
            print(f"âŒ é”™è¯¯: æ•°æ®é›†ç›®å½• {root_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾ç‰‡")
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        img_path = self.images[idx]
        label = self.labels[idx]
        
        # åŠ è½½å›¾ç‰‡
        try:
            image = Image.open(img_path).convert('RGB')
        except Exception as e:
            print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½å›¾ç‰‡ {img_path}: {e}")
            # è¿”å›ä¸€ä¸ªé»‘è‰²å›¾ç‰‡ä½œä¸ºå ä½ç¬¦
            image = Image.new('RGB', (224, 224), (0, 0, 0))
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

def get_data_transforms():
    """è·å–æ•°æ®å¢å¼ºå’Œè½¬æ¢"""
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'test': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }
    return data_transforms

def create_dataloaders(base_dir=None):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print("åŠ è½½æ•°æ®é›†...")
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šåŸºç¡€ç›®å½•ï¼Œä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•
    if base_dir is None:
        base_dir = get_script_dir()
    
    data_dir = os.path.join(base_dir, 'data')
    print(f"æ•°æ®ç›®å½•: {data_dir}")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_dir):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®ç›®å½• {data_dir}")
        print("è¯·å…ˆè¿è¡Œ 01_æ•°æ®å‡†å¤‡.py")
        return None, None
    
    data_transforms = get_data_transforms()
    
    # åˆ›å»ºæ•°æ®é›†è·¯å¾„
    train_dir = os.path.join(data_dir, 'train')
    val_dir = os.path.join(data_dir, 'val')
    test_dir = os.path.join(data_dir, 'test')
    
    print(f"è®­ç»ƒæ•°æ®ç›®å½•: {train_dir}")
    print(f"éªŒè¯æ•°æ®ç›®å½•: {val_dir}")
    print(f"æµ‹è¯•æ•°æ®ç›®å½•: {test_dir}")
    
    # æ£€æŸ¥æ¯ä¸ªç›®å½•æ˜¯å¦å­˜åœ¨
    for dir_path, dir_name in [(train_dir, 'è®­ç»ƒ'), (val_dir, 'éªŒè¯'), (test_dir, 'æµ‹è¯•')]:
        if not os.path.exists(dir_path):
            print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°{dir_name}ç›®å½• {dir_path}")
    
    # åˆ›å»ºæ•°æ®é›†
    datasets = {
        'train': CataractDataset(train_dir, transform=data_transforms['train']),
        'val': CataractDataset(val_dir, transform=data_transforms['val']),
        'test': CataractDataset(test_dir, transform=data_transforms['test'])
    }
    
    # æ£€æŸ¥æ•°æ®é›†å¤§å°
    dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val', 'test']}
    
    if dataset_sizes['train'] == 0 or dataset_sizes['val'] == 0:
        print("âŒ é”™è¯¯: è®­ç»ƒé›†æˆ–éªŒè¯é›†ä¸ºç©º")
        print("è¯·ç¡®ä¿æ•°æ®å‡†å¤‡è„šæœ¬æ­£ç¡®è¿è¡Œï¼Œå¹¶ä¸”å›¾ç‰‡æ ¼å¼æ­£ç¡®")
        return None, None
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    num_workers = 0 if sys.platform == 'win32' else 2  # Windowsä¸Šè®¾ç½®ä¸º0é¿å…é—®é¢˜
    dataloaders = {
        'train': DataLoader(datasets['train'], batch_size=16, shuffle=True, num_workers=num_workers),
        'val': DataLoader(datasets['val'], batch_size=16, shuffle=False, num_workers=num_workers),
        'test': DataLoader(datasets['test'], batch_size=16, shuffle=False, num_workers=num_workers)
    }
    
    print(f"è®­ç»ƒé›†: {dataset_sizes['train']} å¼ å›¾ç‰‡")
    print(f"éªŒè¯é›†: {dataset_sizes['val']} å¼ å›¾ç‰‡")
    print(f"æµ‹è¯•é›†: {dataset_sizes['test']} å¼ å›¾ç‰‡")
    
    return dataloaders, dataset_sizes

def build_resnet18_model(num_classes=2):
    """æ„å»ºResNet18æ¨¡å‹"""
    print("æ„å»ºResNet18æ¨¡å‹...")
    
    try:
        # åŠ è½½é¢„è®­ç»ƒçš„ResNet18
        model = models.resnet18(pretrained=True)
    except Exception as e:
        print(f"âš ï¸ è­¦å‘Š: æ— æ³•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {e}")
        print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        model = models.resnet18(pretrained=False)
    
    # å†»ç»“æ‰€æœ‰å±‚
    for param in model.parameters():
        param.requires_grad = False
    
    # æ›¿æ¢æœ€åä¸€å±‚å…¨è¿æ¥å±‚
    num_features = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Dropout(0.5),
        nn.Linear(num_features, 512),
        nn.ReLU(),
        nn.Dropout(0.3),
        nn.Linear(512, num_classes)
    )
    
    # è§£å†»æœ€åä¸¤å±‚
    for param in model.layer4.parameters():
        param.requires_grad = True
    for param in model.fc.parameters():
        param.requires_grad = True
    
    return model

def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=20):
    """è®­ç»ƒæ¨¡å‹"""
    print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    
    since = time.time()
    
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    
    # è®°å½•è®­ç»ƒå†å²
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': []
    }
    
    for epoch in range(num_epochs):
        print(f'\nEpoch {epoch+1}/{num_epochs}')
        print('-' * 20)
        
        # æ¯ä¸ªepochæœ‰è®­ç»ƒå’ŒéªŒè¯é˜¶æ®µ
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # è®­ç»ƒæ¨¡å¼
                if scheduler is not None:
                    scheduler.step()
            else:
                model.eval()   # éªŒè¯æ¨¡å¼
            
            running_loss = 0.0
            running_corrects = 0
            
            # éå†æ•°æ®
            data_iter = dataloaders[phase]
            if TQDM_AVAILABLE:
                data_iter = tqdm(data_iter, desc=f'{phase.capitalize()}')
            
            for inputs, labels in data_iter:
                inputs = inputs.to(device)
                labels = labels.to(device)
                
                # æ¢¯åº¦æ¸…é›¶
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                    
                    # åå‘ä¼ æ’­+ä¼˜åŒ–åªåœ¨è®­ç»ƒé˜¶æ®µ
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                
                # ç»Ÿè®¡
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            
            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]
            
            # è®°å½•å†å²
            if phase == 'train':
                history['train_loss'].append(epoch_loss)
                history['train_acc'].append(epoch_acc.item() if hasattr(epoch_acc, 'item') else epoch_acc)
            else:
                history['val_loss'].append(epoch_loss)
                history['val_acc'].append(epoch_acc.item() if hasattr(epoch_acc, 'item') else epoch_acc)
            
            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
            
            # æ·±æ‹·è´æœ€ä½³æ¨¡å‹
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
    
    time_elapsed = time.time() - since
    print(f'\nè®­ç»ƒå®Œæˆ! ç”¨æ—¶: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.4f}')
    
    # åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡
    model.load_state_dict(best_model_wts)
    
    return model, history

def plot_training_history(history, save_dir='results'):
    """ç»˜åˆ¶è®­ç»ƒå†å²å›¾è¡¨"""
    print("ç»˜åˆ¶è®­ç»ƒå†å²...")
    
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, 'training_history.png')
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    # æŸå¤±æ›²çº¿
    axes[0].plot(history['train_loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
    axes[0].plot(history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
    axes[0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('æŸå¤±')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡æ›²çº¿
    axes[1].plot(history['train_acc'], label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
    axes[1].plot(history['val_acc'], label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
    axes[1].set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('å‡†ç¡®ç‡')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… è®­ç»ƒå†å²å›¾å·²ä¿å­˜åˆ° {save_path}")
    plt.close()

def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ç™½å†…éšœç­›æŸ¥æ¨¡å‹è®­ç»ƒ")
    print("="*70)
    
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    base_dir = get_script_dir()
    print(f"è„šæœ¬æ‰€åœ¨ç›®å½•: {base_dir}")
    
    # è®¾ç½®è®¾å¤‡
    global device
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºç›®å½•
    models_dir = os.path.join(base_dir, 'models')
    results_dir = os.path.join(base_dir, 'results')
    os.makedirs(models_dir, exist_ok=True)
    os.makedirs(results_dir, exist_ok=True)
    
    # æ£€æŸ¥tqdmæ˜¯å¦å¯ç”¨
    if not TQDM_AVAILABLE:
        print("\nâš ï¸ æ³¨æ„: tqdmæ¨¡å—æœªå®‰è£…ï¼Œè¿›åº¦æ¡æ˜¾ç¤ºå—é™")
        print("   å»ºè®®å®‰è£…ä»¥è·å¾—æ›´å¥½çš„ä½“éªŒ: pip install tqdm")
    
    try:
        # 1. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloaders, dataset_sizes = create_dataloaders(base_dir)
        
        if dataloaders is None or dataset_sizes is None:
            print("âŒ æ— æ³•åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œç¨‹åºé€€å‡º")
            return
        
        # 2. æ„å»ºæ¨¡å‹
        model = build_resnet18_model(num_classes=2)
        model = model.to(device)
        
        # 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)
        
        # 4. å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
        
        # 5. è®­ç»ƒæ¨¡å‹
        model, history = train_model(
            model, dataloaders, dataset_sizes,
            criterion, optimizer, scheduler,
            num_epochs=15  # å‡å°‘epochæ•°ä»¥åŠ å¿«è®­ç»ƒ
        )
        
        # 6. ä¿å­˜æ¨¡å‹
        model_path = os.path.join(models_dir, 'cataract_resnet18.pth')
        torch.save({
            'model_state_dict': model.state_dict(),
            'history': history,
            'classes': ['cataract', 'normal'],
            'input_size': 224
        }, model_path)
        print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")
        
        # 7. ç»˜åˆ¶è®­ç»ƒå†å²
        plot_training_history(history, results_dir)
        
        # 8. åœ¨æµ‹è¯•é›†ä¸Šå¿«é€Ÿæµ‹è¯•
        print("\nåœ¨æµ‹è¯•é›†ä¸Šå¿«é€Ÿæµ‹è¯•...")
        model.eval()
        test_correct = 0
        test_total = 0
        
        with torch.no_grad():
            test_iter = dataloaders['test']
            if TQDM_AVAILABLE:
                test_iter = tqdm(test_iter, desc='æµ‹è¯•')
            
            for inputs, labels in test_iter:
                inputs = inputs.to(device)
                labels = labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
        
        if test_total > 0:
            test_accuracy = 100 * test_correct / test_total
            print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.2f}% ({test_correct}/{test_total})")
        else:
            print("âš ï¸ æµ‹è¯•é›†ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—å‡†ç¡®ç‡")
        
        print("\n" + "="*70)
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print("="*70)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ å»ºè®®:")
        print("1. ç¡®ä¿å·²å®‰è£…å¿…è¦çš„åº“: pip install torch torchvision pillow matplotlib")
        print("2. ç¡®ä¿æ•°æ®å‡†å¤‡è„šæœ¬å·²æ­£ç¡®è¿è¡Œ")
        print("3. æ£€æŸ¥å›¾ç‰‡æ ¼å¼æ˜¯å¦æ”¯æŒ (jpg, pngç­‰)")
        print("4. å¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œå°è¯•å‡å°batch_size")

if __name__ == "__main__":
    main()